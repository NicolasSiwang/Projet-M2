{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bm25s\n",
    "# %pip install spacy\n",
    "# %pip install -U 'spacy[cuda12x]'\n",
    "# %pip install rouge_score\n",
    "# %pip install pysbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as fct\n",
    "import bm25s\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'SCOTUS/train.json'\n",
    "dev_path = 'SCOTUS/dev.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarization with legal-pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"On March 5, 2021, the Securities and Exchange Commission charged AT&T, Inc. with repeatedly violating Regulation FD, and three of its Investor Relations executives with aiding and abetting AT&T's violations, by selectively disclosing material nonpublic information to research analysts. According to the SEC's complaint, AT&T learned in March 2016 that a steeper-than-expected decline in its first quarter smartphone sales would cause AT&T's revenue to fall short of analysts' estimates for the quarter. The complaint alleges that to avoid falling short of the consensus revenue estimate for the third consecutive quarter, AT&T Investor Relations executives Christopher Womack, Michael Black, and Kent Evans made private, one-on-one phone calls to analysts at approximately 20 separate firms. On these calls, the AT&T executives allegedly disclosed AT&T's internal smartphone sales data and the impact of that data on internal revenue metrics, despite the fact that internal documents specifically informed Investor Relations personnel that AT&T's revenue and sales of smartphones were types of information generally considered \"material\" to AT&T investors, and therefore prohibited from selective disclosure under Regulation FD. The complaint further alleges that as a result of what they were told on these calls, the analysts substantially reduced their revenue forecasts, leading to the overall consensus revenue estimate falling to just below the level that AT&T ultimately reported to the public on April 26, 2016. The SEC's complaint, filed in federal district court in Manhattan, charges AT&T with violations of the disclosure provisions of Section 13(a) of the Securities Exchange Act of 1934 and Regulation FD thereunder, and charges Womack, Evans and Black with aiding and abetting these violations. The complaint seeks permanent injunctive relief and civil monetary penalties against each defendant. The SEC's investigation was conducted by George N. Stepaniuk, Thomas Peirce, and David Zetlin-Jones of the SEC's New York Regional Office. The SEC's litigation will be conducted by Alexander M. Vasilescu, Victor Suthammanont, and Mr. Zetlin-Jones. The case is being supervised by Sanjay Wadhwa.\"\"\"\n",
    "\n",
    "summary = fct.summarize(text, \"legal-pegasus\")\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des chemins vers les fichiers JSON d'entraînement et de développement\n",
    "train_path = 'SCOTUS/train.json'\n",
    "dev_path = 'SCOTUS/dev.json'\n",
    "\n",
    "# Ouverture du fichier d'entraînement\n",
    "train = fct.open_file(train_path, \"json\")\n",
    "\n",
    "# Récupération du document source du premier élément de l'ensemble d'entraînement\n",
    "# document = train[0][\"raw_source\"]\n",
    "document = train\n",
    "\n",
    "# Récupération des éléments de la cible (faits, question, conclusion)\n",
    "paragraph_target = (\n",
    "    train[0]['raw_target']['facts_of_the_case'] +\n",
    "    train[0]['raw_target']['question'] +\n",
    "    train[0]['raw_target']['conclusion']\n",
    ")\n",
    "\n",
    "train_path = 'data_txt_save/train_0.txt'\n",
    "train = fct.open_file(train_path, \"txt\")\n",
    "\n",
    "document = train\n",
    "\n",
    "\n",
    "# Segmentation des phrases du document source\n",
    "sentences = fct.sent_segmentation(document, method='custom_spacy')\n",
    "\n",
    "# Résumé des phrases en utilisant le modèle BERT\n",
    "summary = fct.bb25LegalSum(sentences, \"bert-base-uncased\", 5)\n",
    "\n",
    "# Évaluation de la qualité du résumé à l'aide de la métrique ROUGE\n",
    "bb25_rouge = fct.rouge_evaluations(\" \".join(summary), paragraph_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb25_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des chemins vers les fichiers JSON d'entraînement et de développement\n",
    "train_path_json = 'SCOTUS/train.json'\n",
    "dev_path_json = 'SCOTUS/dev.json'\n",
    "\n",
    "# Ouverture du fichier d'entraînement JSON\n",
    "train_json = fct.open_file(train_path_json, \"json\")\n",
    "\n",
    "# Récupération du document source et des éléments de la cible (faits, question, conclusion)\n",
    "document_json = train_json[0][\"raw_source\"]\n",
    "paragraph_target_json = (\n",
    "    train_json[0]['raw_target']['facts_of_the_case'] +\n",
    "    train_json[0]['raw_target']['question'] +\n",
    "    train_json[0]['raw_target']['conclusion']\n",
    ")\n",
    "\n",
    "# Définir le chemin du fichier texte à traiter\n",
    "train_path_txt = 'data_txt_save/train_0.txt'\n",
    "\n",
    "# Ouverture du fichier d'entraînement TXT\n",
    "document_txt = fct.open_file(train_path_txt, \"txt\")\n",
    "\n",
    "# Liste des méthodes de segmentation et des modèles à tester\n",
    "methods = ['nltk', 'spacy', 'pySBD']\n",
    "model = \"bert-base-uncased\"  # Modèle de résumé à utiliser\n",
    "results = {}  # Dictionnaire pour stocker les résultats\n",
    "\n",
    "# Fonction pour évaluer les modèles\n",
    "def evaluate_models(document, paragraph_target, file_type):\n",
    "    for method in methods:\n",
    "        # Segmentation des phrases du document source\n",
    "        sentences = fct.sent_segmentation(document, method=method)\n",
    "        \n",
    "        # Résumé des phrases\n",
    "        summary = fct.bb25LegalSum(sentences, model, 5)\n",
    "        \n",
    "        # Évaluation de la qualité du résumé à l'aide de la métrique ROUGE\n",
    "        bb25_rouge = fct.rouge_evaluations(\" \".join(summary), paragraph_target)\n",
    "        \n",
    "        # Stockage des résultats dans le dictionnaire\n",
    "        results[f\"{file_type} - {method}\"] = bb25_rouge\n",
    "\n",
    "# Évaluation des modèles pour le fichier JSON\n",
    "evaluate_models(document_json, paragraph_target_json, \"JSON\")\n",
    "\n",
    "# Évaluation des modèles pour le fichier TXT\n",
    "evaluate_models(document_txt, paragraph_target_json, \"TXT\")  # Utilise le même paragraphe cible\n",
    "\n",
    "# Affichage des résultats de comparaison\n",
    "for method, score in results.items():\n",
    "    print(f\"Méthode: {method}, Score ROUGE: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Method  Metric  Precision    Recall  F1-Score\n",
      "0   JSON - nltk  rouge1   0.449275  0.038871  0.071552\n",
      "1   JSON - nltk  rouge2   0.109091  0.009407  0.017321\n",
      "2   JSON - nltk  rougeL   0.184783  0.015987  0.029429\n",
      "0  JSON - spacy  rouge1   0.094203  0.292135  0.142466\n",
      "1  JSON - spacy  rouge2   0.025455  0.079545  0.038567\n",
      "2  JSON - spacy  rougeL   0.076087  0.235955  0.115068\n",
      "0  JSON - pySBD  rouge1   0.086957  0.279070  0.132597\n",
      "1  JSON - pySBD  rouge2   0.010909  0.035294  0.016667\n",
      "2  JSON - pySBD  rougeL   0.050725  0.162791  0.077348\n",
      "0    TXT - nltk  rouge1   0.257246  0.467105  0.331776\n",
      "1    TXT - nltk  rouge2   0.080000  0.145695  0.103286\n",
      "2    TXT - nltk  rougeL   0.126812  0.230263  0.163551\n",
      "0   TXT - spacy  rouge1   0.271739  0.563910  0.366748\n",
      "1   TXT - spacy  rouge2   0.090909  0.189394  0.122850\n",
      "2   TXT - spacy  rougeL   0.126812  0.263158  0.171149\n",
      "0   TXT - pySBD  rouge1   0.474638  0.587444  0.525050\n",
      "1   TXT - pySBD  rouge2   0.167273  0.207207  0.185111\n",
      "2   TXT - pySBD  rougeL   0.202899  0.251121  0.224449\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Exemple de données de score ROUGE pour chaque méthode\n",
    "results = {\n",
    "    \"JSON - nltk\": {\n",
    "        \"Metric\": [\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "        \"Precision\": [0.449275, 0.109091, 0.184783],\n",
    "        \"Recall\": [0.038871, 0.009407, 0.015987],\n",
    "        \"F1-Score\": [0.071552, 0.017321, 0.029429],\n",
    "    },\n",
    "    \"JSON - spacy\": {\n",
    "        \"Metric\": [\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "        \"Precision\": [0.094203, 0.025455, 0.076087],\n",
    "        \"Recall\": [0.292135, 0.079545, 0.235955],\n",
    "        \"F1-Score\": [0.142466, 0.038567, 0.115068],\n",
    "    },\n",
    "    \"JSON - pySBD\": {\n",
    "        \"Metric\": [\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "        \"Precision\": [0.086957, 0.010909, 0.050725],\n",
    "        \"Recall\": [0.279070, 0.035294, 0.162791],\n",
    "        \"F1-Score\": [0.132597, 0.016667, 0.077348],\n",
    "    },\n",
    "    \"TXT - nltk\": {\n",
    "        \"Metric\": [\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "        \"Precision\": [0.257246, 0.080000, 0.126812],\n",
    "        \"Recall\": [0.467105, 0.145695, 0.230263],\n",
    "        \"F1-Score\": [0.331776, 0.103286, 0.163551],\n",
    "    },\n",
    "    \"TXT - spacy\": {\n",
    "        \"Metric\": [\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "        \"Precision\": [0.271739, 0.090909, 0.126812],\n",
    "        \"Recall\": [0.563910, 0.189394, 0.263158],\n",
    "        \"F1-Score\": [0.366748, 0.122850, 0.171149],\n",
    "    },\n",
    "    \"TXT - pySBD\": {\n",
    "        \"Metric\": [\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "        \"Precision\": [0.474638, 0.167273, 0.202899],\n",
    "        \"Recall\": [0.587444, 0.207207, 0.251121],\n",
    "        \"F1-Score\": [0.525050, 0.185111, 0.224449],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Créer un DataFrame pour chaque méthode et les concaténer\n",
    "dfs = []\n",
    "for key, value in results.items():\n",
    "    df = pd.DataFrame(value)\n",
    "    df['Method'] = key\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concaténer tous les DataFrames\n",
    "final_df = pd.concat(dfs)\n",
    "\n",
    "# Réorganiser les colonnes pour une meilleure lisibilité\n",
    "final_df = final_df[['Method', 'Metric', 'Precision', 'Recall', 'F1-Score']]\n",
    "\n",
    "# Affichage des résultats\n",
    "print(final_df)\n",
    "\n",
    "# Optionnel: Enregistrer le résultat dans un fichier CSV\n",
    "final_df.to_csv('comparaison_models.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
