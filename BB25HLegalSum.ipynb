{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bm25s\n",
    "# %pip install spacy\n",
    "# %pip install -U 'spacy[cuda12x]'\n",
    "# %pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as fct\n",
    "import custom_spacy as csp\n",
    "import spacy\n",
    "import bm25s\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'SCOTUS/train.json'\n",
    "dev_path = 'SCOTUS/dev.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarization with legal-pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"On March 5, 2021, the Securities and Exchange Commission charged AT&T, Inc. with repeatedly violating Regulation FD, and three of its Investor Relations executives with aiding and abetting AT&T's violations, by selectively disclosing material nonpublic information to research analysts. According to the SEC's complaint, AT&T learned in March 2016 that a steeper-than-expected decline in its first quarter smartphone sales would cause AT&T's revenue to fall short of analysts' estimates for the quarter. The complaint alleges that to avoid falling short of the consensus revenue estimate for the third consecutive quarter, AT&T Investor Relations executives Christopher Womack, Michael Black, and Kent Evans made private, one-on-one phone calls to analysts at approximately 20 separate firms. On these calls, the AT&T executives allegedly disclosed AT&T's internal smartphone sales data and the impact of that data on internal revenue metrics, despite the fact that internal documents specifically informed Investor Relations personnel that AT&T's revenue and sales of smartphones were types of information generally considered \"material\" to AT&T investors, and therefore prohibited from selective disclosure under Regulation FD. The complaint further alleges that as a result of what they were told on these calls, the analysts substantially reduced their revenue forecasts, leading to the overall consensus revenue estimate falling to just below the level that AT&T ultimately reported to the public on April 26, 2016. The SEC's complaint, filed in federal district court in Manhattan, charges AT&T with violations of the disclosure provisions of Section 13(a) of the Securities Exchange Act of 1934 and Regulation FD thereunder, and charges Womack, Evans and Black with aiding and abetting these violations. The complaint seeks permanent injunctive relief and civil monetary penalties against each defendant. The SEC's investigation was conducted by George N. Stepaniuk, Thomas Peirce, and David Zetlin-Jones of the SEC's New York Regional Office. The SEC's litigation will be conducted by Alexander M. Vasilescu, Victor Suthammanont, and Mr. Zetlin-Jones. The case is being supervised by Sanjay Wadhwa.\"\"\"\n",
    "\n",
    "summary = fct.summarize(text, \"legal-pegasus\")\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BM25 with custom_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified spacy for segmentation of legal text\n",
    "nlp = csp.custom_spacy_model()            \n",
    "\n",
    "# Process the original text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Create a corpus for BM25\n",
    "corpus = [sent.text for sent in doc.sents]  \n",
    "\n",
    "# Initialize and index BM25\n",
    "retriever = bm25s.BM25(corpus=corpus)\n",
    "retriever.index(bm25s.tokenize(corpus))  \n",
    "\n",
    "# Process the summary\n",
    "doc_sum = nlp(summary)\n",
    "\n",
    "# Retrieve sentences\n",
    "for sent in doc_sum.sents:\n",
    "    query = sent.text\n",
    "    results, scores = retriever.retrieve(bm25s.tokenize(query), k=2)\n",
    "    avg_score = np.mean(scores)\n",
    "    print(sent.text + \" ; \" + str(avg_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text), len(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = fct.rouge_evaluations(text, summary)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle et le tokenizer BERT pré-entraîné\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de texte juridique\n",
    "# document = \"\"\"\n",
    "# The law guarantees the fundamental rights and freedoms of every citizen.\n",
    "# The judge rendered his verdict based on the 2023 law.\n",
    "# The rights of citizens must be respected in all legal proceedings.\n",
    "# The 1958 constitution is the basis of the current legal system.\n",
    "# Every individual has the right to a fair trial under applicable laws.\n",
    "# \"\"\"\n",
    "\n",
    "with open(train_path, 'r', encoding=\"utf-8\") as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "document = train[0][\"raw_source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation en phrases\n",
    "sentences = sent_tokenize(document)\n",
    "\n",
    "# Fonction pour obtenir les embeddings de phrases avec BERT\n",
    "def get_sentence_embeddings(sentences, tokenizer, model):\n",
    "    \"\"\"Obtenir les embeddings de phrases avec BERT\n",
    "    Args:\n",
    "        sentences (List[str]): Liste des phrases à encoder\n",
    "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer BERT\n",
    "        model (transformers.PreTrainedModel): Modèle BERT\n",
    "    Returns:\n",
    "        np.array: Tableau des embeddings de phrases    \n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy())  # Moyenne des embeddings\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Obtenir les embeddings des phrases\n",
    "sentence_embeddings = get_sentence_embeddings(sentences, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser K-Means pour regrouper des phrases similaires\n",
    "n_clusters = 5  # Nombre de clusters souhaités (peut être ajusté)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans.fit(sentence_embeddings)\n",
    "\n",
    "# Étiquettes des clusters\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = {}\n",
    "for i in range(n_clusters):\n",
    "    print(f\"\\nCluster {i+1}:\")\n",
    "    cluster[i] = []  # Initialiser une liste pour chaque cluster\n",
    "    for j, sentence in enumerate(sentences):\n",
    "        if labels[j] == i:\n",
    "            print(f\"- {sentence}\")\n",
    "            # Ajouter 'sentence' au cluster 'i'\n",
    "            cluster[i].append(sentence)\n",
    "    # Afficher les clés du dictionnaire 'cluster'\n",
    "print(cluster.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer le score de silhouette pour évaluer la qualité du clustering\n",
    "silhouette_avg = silhouette_score(sentence_embeddings, labels)\n",
    "print(f\"\\nSilhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer la bibliothèque si nécessaire\n",
    "# %pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de documents (textes juridiques)\n",
    "# documents = [\n",
    "#     \"a cat is a feline and likes to purr\",\n",
    "#     \"a dog is the human's best friend and loves to play\",\n",
    "#     \"a bird is a beautiful animal that can fly\",\n",
    "#     \"a fish is a creature that lives in water and swims\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour stocker les documents tokenisés par cluster\n",
    "tokenized_clusters = {}\n",
    "\n",
    "# Tokenisation des documents pour chaque cluster\n",
    "for i, sentences in cluster.items():\n",
    "    tokenized_clusters[i] = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Initialiser un modèle BM25 pour chaque cluster\n",
    "bm25_models = {}\n",
    "for i, tokenized_docs in tokenized_clusters.items():\n",
    "    bm25_models[i] = BM25Okapi(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de requête\n",
    "query = \"law and legal rights\"\n",
    "tokenized_query = word_tokenize(query.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des scores BM25 pour tous les clusters\n",
    "for cluster_id, bm25 in bm25_models.items():\n",
    "    # Calcul des scores pour la requête dans chaque cluster\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    print(f\"Scores pour la requête dans le cluster {cluster_id}: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id, bm25 in bm25_models.items():\n",
    "    # Récupérer les documents les plus pertinents pour la requête\n",
    "    top_docs = bm25.get_top_n(tokenized_query, tokenized_clusters[cluster_id], n=1) # n=1 peut être ajusté pour obtenir plus de documents\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    for doc in top_docs:\n",
    "        sentence = ' '.join(doc)\n",
    "        print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
