{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarization with legal-pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as fct\n",
    "import bm25s\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'SCOTUS/train.json'\n",
    "dev_path = 'SCOTUS/dev.json'\n",
    "train_path_base = 'data_txt_save/train_'\n",
    "clean_path_base = 'clean_data_txt_save/train_'\n",
    "\n",
    "document = fct.open_file(train_path, \"json\")\n",
    "\n",
    "# Boucle sur les 10 premiers textes\n",
    "for i in range(10):\n",
    "    # Construire les chemins pour chaque fichier en fonction de l'index\n",
    "    train_path_txt = f'{train_path_base}{i}.txt'\n",
    "    clean_path_txt = f'{clean_path_base}{i}.txt'\n",
    "    train = document[i][\"raw_source\"]\n",
    "\n",
    "    # Ouvre les fichiers correspondants\n",
    "    document_txt = fct.open_file(train_path_txt, \"txt\")\n",
    "    clean_document_txt = fct.open_file(clean_path_txt, \"txt\")\n",
    "    \n",
    "    # Résumer les fichiers\n",
    "    summary_txt = fct.summarize(document_txt, \"legal-pegasus\", max_length=1000)\n",
    "    summary_clean_txt = fct.summarize(clean_document_txt, \"legal-pegasus\", max_length=1000)\n",
    "    summary_raw = fct.summarize(train, \"legal-pegasus\", max_length=1000)\n",
    "\n",
    "    # Afficher les résumés\n",
    "    print(f\"Summary TXT {i+1}:\", summary_txt)\n",
    "    print(f\"Summary Clean TXT {i+1}:\", summary_clean_txt)\n",
    "    print(f\"Summary Raw {i+1}:\", summary_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chemins des fichiers\n",
    "train_path = 'SCOTUS/train.json'\n",
    "train_path_base = 'data_txt_save/train_'\n",
    "clean_path_base = 'clean_data_txt_save/train_'\n",
    "\n",
    "# Ouvre le fichier JSON\n",
    "document = fct.open_file(train_path, \"json\")\n",
    "\n",
    "# Stocker les données sous forme de DataFrame\n",
    "results = []\n",
    "execution_times = {}\n",
    "\n",
    "# Boucle sur les 10 premiers textes\n",
    "for i in range(10):\n",
    "    # Construire les chemins pour chaque fichier\n",
    "    train_path_txt = f'{train_path_base}{i}.txt'\n",
    "    clean_path_txt = f'{clean_path_base}{i}.txt'\n",
    "    \n",
    "    # Ouvrir les fichiers correspondants\n",
    "    document_txt = fct.open_file(train_path_txt, \"txt\")\n",
    "    clean_document_txt = fct.open_file(clean_path_txt, \"txt\")\n",
    "    train = document[i][\"raw_source\"]\n",
    "    \n",
    "    # Résumer les fichiers\n",
    "    summary_txt = fct.summarize(document_txt, \"legal-pegasus\", max_length=1000)\n",
    "    summary_clean_txt = fct.summarize(clean_document_txt, \"legal-pegasus\", max_length=1000)\n",
    "    summary_raw = fct.summarize(train, \"legal-pegasus\", max_length=1000)\n",
    "\n",
    "    # Calculer les longueurs des résumés comme une métrique simple\n",
    "    len_summary_txt = len(summary_txt)\n",
    "    len_summary_clean_txt = len(summary_clean_txt)\n",
    "    len_summary_raw = len(summary_raw)\n",
    "\n",
    "    # Ajouter les résultats à la liste\n",
    "    results.append({\n",
    "        'Text ID': i,\n",
    "        'Method': 'TXT',\n",
    "        'Summary Length': len_summary_txt,\n",
    "        'Summary': summary_txt\n",
    "    })\n",
    "    results.append({\n",
    "        'Text ID': i,\n",
    "        'Method': 'Clean TXT',\n",
    "        'Summary Length': len_summary_clean_txt,\n",
    "        'Summary': summary_clean_txt\n",
    "    })\n",
    "    results.append({\n",
    "        'Text ID': i,\n",
    "        'Method': 'Raw',\n",
    "        'Summary Length': len_summary_raw,\n",
    "        'Summary': summary_raw\n",
    "    })\n",
    "\n",
    "    break\n",
    "\n",
    "# Convertir les résultats en DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Réarranger les colonnes\n",
    "df = df[['Text ID', 'Method', 'Summary Length', 'Summary']]\n",
    "\n",
    "# Fonction de surbrillance pour les longueurs de résumés\n",
    "def highlight_length(df):\n",
    "    styles = pd.DataFrame('', index=df.index, columns=df.columns)\n",
    "\n",
    "    # Appliquer la surbrillance pour les 3 plus longues et 3 plus courtes longueurs de résumés\n",
    "    top_3_max = df['Summary Length'].nlargest(3)\n",
    "    top_3_min = df['Summary Length'].nsmallest(3)\n",
    "\n",
    "    # Dégradé vert pour les plus longs résumés\n",
    "    for i in df.index:\n",
    "        if df['Summary Length'].iloc[i] in top_3_max.values:\n",
    "            rank = top_3_max.rank(ascending=False)[top_3_max == df['Summary Length'].iloc[i]].values[0]\n",
    "            alpha = 1 - (rank - 1) / 3\n",
    "            styles.loc[i, 'Summary Length'] = f'background-color: rgba(50, 200, 50, {alpha});'\n",
    "\n",
    "    # Dégradé rouge pour les plus courts résumés\n",
    "    for i in df.index:\n",
    "        if df['Summary Length'].iloc[i] in top_3_min.values:\n",
    "            rank = top_3_min.rank()[top_3_min == df['Summary Length'].iloc[i]].values[0]\n",
    "            alpha = 1 - (rank - 1) / 3  \n",
    "            styles.loc[i, 'Summary Length'] = f'background-color: rgba(200, 50, 50, {alpha});'\n",
    "\n",
    "    return styles\n",
    "\n",
    "# Appliquer le style au DataFrame\n",
    "styled_df = df.style.apply(highlight_length, axis=None)\n",
    "\n",
    "# Afficher le DataFrame stylisé\n",
    "styled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import functions as fct\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "train_path = 'dataset_legal-pegasus/dataset/UK-Abs/train-data'\n",
    "test_path = 'dataset_legal-pegasus/dataset/UK-Abs/test-data'\n",
    "\n",
    "train_path_txt = train_path + '/judgement'\n",
    "train_path_summary = train_path + '/summary'\n",
    "test_path_txt = test_path + '/judgement'\n",
    "test_path_summary = test_path + '/summary'\n",
    "\n",
    "# Parcourir les fichiers dans le dossier de train et de test\n",
    "train_files = os.listdir(train_path_txt)\n",
    "test_files = os.listdir(test_path_txt)\n",
    "\n",
    "# Charger le modèle Legal-Pegasus et le tokenizer\n",
    "model_name = \"nsi319/legal-pegasus\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour diviser le document en segments\n",
    "def chunk_text(text, chunk_size=1024):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=chunk_size, truncation=True, padding=True)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for i, word in enumerate(text.split()):\n",
    "        if current_length + len(word) <= chunk_size:\n",
    "            current_chunk.append(word)\n",
    "            current_length += len(word) + 1\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word) + 1\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summary = fct.open_file(train_path_txt + '/' + train_files[0], 'txt')\n",
    "print(len(reference_summary))\n",
    "\n",
    "# Générer le résumé\n",
    "chunks = chunk_text(reference_summary)\n",
    "print(len(chunks))\n",
    "\n",
    "# generated_summary = [fct.summarize(chunk, \"legal-pegasus\", max_length=1000) for chunk in chunks]\n",
    "generated_summary = fct.summarize(chunks[0], \"legal-pegasus\", max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Résumé de référence : \", reference_summary)\n",
    "print(\"Résumé généré : \", generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# Fonction pour calculer les scores ROUGE\n",
    "def compute_rouge_scores(reference_summary, generated_summary):\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    # Calcul des scores ROUGE-1, ROUGE-2 et ROUGE-L\n",
    "    scores = rouge.get_scores(generated_summary, reference_summary)\n",
    "    \n",
    "    # S'assurer que des scores ont été calculés avant de procéder\n",
    "    if len(scores) == 0:\n",
    "        return {\"error\": \"No scores could be computed. Check your input summaries.\"}\n",
    "    \n",
    "    # Calcul de la moyenne des scores\n",
    "    avg_scores = {\n",
    "        'rouge-1': {\n",
    "            'precision': sum([score['rouge-1']['p'] for score in scores]) / len(scores) if len(scores) > 0 else 0,\n",
    "            'recall': sum([score['rouge-1']['r'] for score in scores]) / len(scores) if len(scores) > 0 else 0,\n",
    "            'f1-score': sum([score['rouge-1']['f'] for score in scores]) / len(scores) if len(scores) > 0 else 0,\n",
    "        },\n",
    "        'rouge-2': {\n",
    "            'precision': sum([score['rouge-2']['p'] for score in scores]) / len(scores) if len(scores) > 0 else 0,\n",
    "            'recall': sum([score['rouge-2']['r'] for score in scores]) / len(scores) if len(scores) > 0 else 0,\n",
    "            'f1-score': sum([score['rouge-2']['f'] for score in scores]) / len(scores) if len(scores) > 0 else 0,\n",
    "        },\n",
    "        'rouge-l': {\n",
    "            'precision': sum([score['rouge-l']['p'] for score in scores]) / len(scores) if len(scores) > 0 else 0,\n",
    "            'recall': sum([score['rouge-l']['r'] for score in scores]) / len(scores) if len(scores) > 0 else 0,\n",
    "            'f1-score': sum([score['rouge-l']['f'] for score in scores]) / len(scores) if len(scores) > 0 else 0,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return avg_scores\n",
    "\n",
    "# Calcul des scores ROUGE\n",
    "rouge_scores = compute_rouge_scores(reference_summary, generated_summary)\n",
    "\n",
    "# Vérification et affichage des scores\n",
    "if \"error\" in rouge_scores:\n",
    "    print(rouge_scores[\"error\"])\n",
    "else:\n",
    "    print(\"ROUGE-1: \", rouge_scores['rouge-1'])\n",
    "    print(\"ROUGE-2: \", rouge_scores['rouge-2'])\n",
    "    print(\"ROUGE-L: \", rouge_scores['rouge-l'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Calculer le score BERTScore\n",
    "P, R, F1 = score(generated_summary, reference_summary, lang=\"en\", verbose=True)\n",
    "\n",
    "# Afficher les scores de Précision, Rappel et F1\n",
    "print(f\"Précision (P): {P.mean().item():.4f}\")\n",
    "print(f\"Rappel (R): {R.mean().item():.4f}\")\n",
    "print(f\"F1-Score: {F1.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
