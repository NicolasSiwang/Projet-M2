{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a842a0a",
   "metadata": {
    "id": "8a842a0a"
   },
   "source": [
    "### Script to generate summaries using chunking based Pegasus approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb972436",
   "metadata": {
    "id": "cb972436"
   },
   "outputs": [],
   "source": [
    "dataset = \"IN-Abs\" # Options: IN - IN-Abs, UK-UK-Abs, N2-IN-Ext\n",
    "output_path = \"./output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a83915a",
   "metadata": {
    "id": "1a83915a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from utilities import *\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "049e7a6f",
   "metadata": {
    "id": "049e7a6f"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591966a",
   "metadata": {
    "id": "0591966a"
   },
   "outputs": [],
   "source": [
    "#Reading the test documents\n",
    "names, data_source, data_summary = get_summary_data(dataset, \"test\")\n",
    "print(len(names))\n",
    "print(len(data_source))\n",
    "print(len(data_summary))\n",
    "# len_dic = dict_names = get_req_len_dict(dataset, \"test\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b68b70",
   "metadata": {
    "id": "14b68b70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sur : cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Test sur : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0596cefd",
   "metadata": {
    "id": "0596cefd"
   },
   "outputs": [],
   "source": [
    "# Loading Model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nsi319/legal-pegasus\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"nsi319/legal-pegasus\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49734483",
   "metadata": {
    "id": "49734483"
   },
   "outputs": [],
   "source": [
    "def summerize(text, max_len, min_len):\n",
    "    '''\n",
    "    Function to generate summary using Pegasus\n",
    "    input:  nested_sentences - chunks\n",
    "            max_l - Maximum length\n",
    "            min_l - Minimum length\n",
    "    output: document summary\n",
    "    '''\n",
    "    try:\n",
    "        input_tokenized = tokenizer.encode(text, return_tensors='pt',max_length=512,truncation=True).to(device)\n",
    "        summary_ids = model.generate(input_tokenized,\n",
    "                                          num_beams=9,\n",
    "                                          length_penalty=0.1,\n",
    "                                          min_length=min_len,\n",
    "                                          max_length=max_len,\n",
    "                                    )\n",
    "        summary = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0]\n",
    "        return summary\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7f05633",
   "metadata": {
    "id": "e7f05633"
   },
   "outputs": [],
   "source": [
    "def summerize_doc(nested_sentences, p):\n",
    "    '''\n",
    "    Function to generate summary using chunking based Pegasus\n",
    "    input:  nested_sentences - chunks\n",
    "            p - Number of words in summaries per word in the document\n",
    "    output: document summary\n",
    "    '''\n",
    "    device = 'cuda'\n",
    "    result = []\n",
    "    for nested in nested_sentences:\n",
    "        l = int(p * len(nested.split(\" \")))\n",
    "        max_len = l\n",
    "        min_len = l-5\n",
    "        result.append(summerize(nested, max_len, min_len))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0385a1",
   "metadata": {
    "id": "fd0385a1"
   },
   "outputs": [],
   "source": [
    "done_files = glob.glob(output_path + \"*.txt\")\n",
    "done_files = [i[i.rfind(\"/\")+1:] for i in done_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da2186e",
   "metadata": {
    "id": "0da2186e"
   },
   "outputs": [],
   "source": [
    "# main loop to generate and save summaries of each document in the test dataset\n",
    "for i in range(len(data_source)):\n",
    "    done_files = glob.glob(output_path + \"*.txt\")\n",
    "    done_files = [i[i.rfind(\"/\")+1:] for i in done_files]\n",
    "    name = names[i]\n",
    "    if name in done_files:continue\n",
    "    doc = data_source[i]\n",
    "    input_len = len(doc.split(\" \"))\n",
    "    req_len = 512\n",
    "    print(req_len)\n",
    "    print(str(i) + \": \" + name +  \" - \" + str(input_len) + \" : \" + str(req_len), end = \", \")\n",
    "    \n",
    "    nested = nest_sentences(doc,512)\n",
    "    p = float(req_len/input_len)\n",
    "    print(p)\n",
    "    abs_summ = summerize_doc(nested,p)\n",
    "    abs_summ = \" \".join(abs_summ)\n",
    "    print(len((abs_summ.split(\" \"))))\n",
    "    \n",
    "    if len(abs_summ.split(\" \")) > req_len:\n",
    "        abs_summ = abs_summ.split(\" \")\n",
    "        abs_summ = abs_summ[:req_len]\n",
    "        abs_summ = \" \".join(abs_summ)\n",
    "    print(len((abs_summ.split(\" \"))))\n",
    "    path = output_path + name\n",
    "    file = open(path,'w')\n",
    "    file.write(abs_summ)\n",
    "    file.close()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2deb6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "from transformers import logging\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def rouge_evaluations(text, ref, f1_only=True):\n",
    "    \"\"\"Return a dataframe for rouge scores\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(text, ref)\n",
    "\n",
    "    if f1_only:\n",
    "        return {\n",
    "            'rouge1': scores['rouge1'].fmeasure,\n",
    "            'rouge2': scores['rouge2'].fmeasure,\n",
    "            'rougeL': scores['rougeL'].fmeasure,\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'rouge1': scores['rouge1'],\n",
    "            'rouge2': scores['rouge2'],\n",
    "            'rougeL': scores['rougeL'],\n",
    "        }\n",
    "\n",
    "def bert_evaluation(summary, ref, f1_only=True):\n",
    "    \"\"\"Return a dataframe for bert score\"\"\"\n",
    "    # Temporarily set verbosity to ERROR to suppress warnings\n",
    "    logging.set_verbosity_error()\n",
    "\n",
    "    try:\n",
    "        scorer = BERTScorer(lang=\"en\")\n",
    "        precision, recall, f1 = scorer.score([summary], [ref])\n",
    "        \n",
    "        if f1_only:\n",
    "            return f1.mean().item()\n",
    "        else:\n",
    "            return [precision.mean().item(), recall.mean().item(), f1.mean().item()]\n",
    "    finally:\n",
    "        logging.set_verbosity_warning()\n",
    "\n",
    "def evaluations(text, ref, f1_only=True):\n",
    "    \"\"\"Return the different metrics results \\n\n",
    "        f1_only return only the f1_score of each metrics  \"\"\"\n",
    "    rouges = rouge_evaluations(text, ref, f1_only)\n",
    "    bert = bert_evaluation(text, ref, f1_only)\n",
    "    \n",
    "    if f1_only:\n",
    "        data = {\n",
    "            'rouge1': [rouges['rouge1']],\n",
    "            'rouge2': [rouges['rouge2']],\n",
    "            'rougeL': [rouges['rougeL']],\n",
    "            'bert_score': [bert]\n",
    "        }\n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        data = {\n",
    "            'rouge1_P': [rouges['rouge1'].precision],\n",
    "            'rouge1_R': [rouges['rouge1'].recall],\n",
    "            'rouge1_F1': [rouges['rouge1'].fmeasure],\n",
    "            \n",
    "            'rouge2_P': [rouges['rouge2'].precision],\n",
    "            'rouge2_R': [rouges['rouge2'].recall],\n",
    "            'rouge2_F1': [rouges['rouge2'].fmeasure],\n",
    "            \n",
    "            'rougeL_P': [rouges['rougeL'].precision],\n",
    "            'rougeL_R': [rouges['rougeL'].recall],\n",
    "            'rougeL_F1': [rouges['rougeL'].fmeasure],\n",
    "            \n",
    "            'bert_score_P': [bert[0]],\n",
    "            'bert_score_R': [bert[1]],\n",
    "            'bert_score_F1': [bert[2]]\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means :\n",
      "rouge1        0.499684\n",
      "rouge2        0.255940\n",
      "rougeL        0.262419\n",
      "bert_score    0.842430\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Paths to the reference and output folders\n",
    "ref_folder = \"../../dataset/\"+dataset+\"/test-data/summary\"\n",
    "output_folder = \"./output/judgement\"\n",
    "\n",
    "# Function to evaluate all files\n",
    "def evaluate_all(ref_folder, output_folder, f1_only=True):\n",
    "    # Get the list of files in the folders\n",
    "    ref_files = sorted(os.listdir(ref_folder))\n",
    "    output_files = sorted(os.listdir(output_folder))\n",
    "\n",
    "    # Ensure both folders have the same number of files and matching names\n",
    "    assert len(ref_files) == len(output_files), \"Folders have a different number of files\"\n",
    "    assert ref_files == output_files, \"File names do not match between folders\"\n",
    "\n",
    "    # Iterate over each file and evaluate\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    for file_name in ref_files:\n",
    "        ref_path = os.path.join(ref_folder, file_name)\n",
    "        output_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "        # Read the contents of the files\n",
    "        with open(ref_path, 'r', encoding='utf-8') as ref_file:\n",
    "            ref_text = ref_file.read()\n",
    "        with open(output_path, 'r', encoding='utf-8') as output_file:\n",
    "            output_text = output_file.read()\n",
    "\n",
    "        # Call the evaluations function\n",
    "        result = evaluations(output_text, ref_text, f1_only=f1_only)\n",
    "        result[\"file_name\"] = file_name \n",
    "\n",
    "        # Append the dataframe to the results dataframe\n",
    "        results = pd.concat([results, result], ignore_index=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "results = evaluate_all(ref_folder, output_folder)\n",
    "\n",
    "metrics = [col for col in results.columns if col in ['rouge1', 'rouge2', 'rougeL', 'bert_score', 'Execution time']]\n",
    "means = results[metrics].mean()\n",
    "\n",
    "print(\"Means :\")\n",
    "print(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40da7750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.618557</td>\n",
       "      <td>0.263566</td>\n",
       "      <td>0.301546</td>\n",
       "      <td>0.859580</td>\n",
       "      <td>1181.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.522541</td>\n",
       "      <td>0.205339</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.829442</td>\n",
       "      <td>1195.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.542714</td>\n",
       "      <td>0.256927</td>\n",
       "      <td>0.248744</td>\n",
       "      <td>0.844111</td>\n",
       "      <td>1329.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.393893</td>\n",
       "      <td>0.140888</td>\n",
       "      <td>0.192366</td>\n",
       "      <td>0.831265</td>\n",
       "      <td>1378.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.612059</td>\n",
       "      <td>0.342075</td>\n",
       "      <td>0.323094</td>\n",
       "      <td>0.862031</td>\n",
       "      <td>1406.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rouge1    rouge2    rougeL  bert_score file_name\n",
       "0  0.618557  0.263566  0.301546    0.859580  1181.txt\n",
       "1  0.522541  0.205339  0.245902    0.829442  1195.txt\n",
       "2  0.542714  0.256927  0.248744    0.844111  1329.txt\n",
       "3  0.393893  0.140888  0.192366    0.831265  1378.txt\n",
       "4  0.612059  0.342075  0.323094    0.862031  1406.txt"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pegasus-test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
