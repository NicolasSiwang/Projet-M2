{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/content/drive/MyDrive/SCOTU_data_txt_save'\n",
    "test_path = '/content/drive/MyDrive/SCOTU_data_txt_save'\n",
    "\n",
    "train_path_txt = train_path + '/text'\n",
    "train_path_summary = train_path + '/summary'\n",
    "test_path_txt = test_path + '/text_dev'\n",
    "test_path_summary = test_path + '/summary_dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement sur : cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Vérification de la disponibilité du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Entraînement sur : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_data(txt_path, summary_path):\n",
    "    \"\"\"Charge les fichiers texte et résumés pour créer un Dataset.\"\"\"\n",
    "    texts = []\n",
    "    summaries = []\n",
    "\n",
    "    # Charger les fichiers de texte\n",
    "    for file_name in os.listdir(txt_path):\n",
    "        with open(os.path.join(txt_path, file_name), 'r', encoding='utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "\n",
    "    # Charger les fichiers de résumés\n",
    "    for file_name in os.listdir(summary_path):\n",
    "        with open(os.path.join(summary_path, file_name), 'r', encoding='utf-8') as f:\n",
    "            summaries.append(f.read())\n",
    "    # Créer un Dataset Hugging Face\n",
    "    data = {\"text\": texts, \"summary\": summaries}\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "# Charger les données\n",
    "train_dataset = load_data(train_path_txt, train_path_summary)\n",
    "test_dataset = load_data(test_path_txt, test_path_summary)\n",
    "\n",
    "# Vérifier les exemples\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "# Initialiser le tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    \"\"\"Tokenisation des données pour BART.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        examples[\"text\"], max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"summary\"], max_length=256, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Prétraitement des datasets\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# Supprimer les colonnes non nécessaires après prétraitement\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Charger le modèle BART\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "model.to(device)\n",
    "\n",
    "# Configurer les paramètres d'entraînement\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",       # Dossier pour sauvegarder les résultats\n",
    "    evaluation_strategy=\"epoch\", # Évaluation après chaque époque\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",         # Dossier pour les logs\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    fp16=torch.cuda.is_available(),  # Utiliser le calcul en 16 bits si le GPU le permet\n",
    ")\n",
    "\n",
    "# Créer l'entraîneur\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Lancer l'entraînement\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for example in dataset:\n",
    "        inputs = tokenizer(example[\"text\"], return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], max_length=256, num_beams=4, early_stopping=True)\n",
    "        prediction = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        reference = example[\"summary\"]\n",
    "\n",
    "        predictions.append(prediction)\n",
    "        references.append(reference)\n",
    "    \n",
    "    return metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Évaluer le modèle\n",
    "results = evaluate_model(model, tokenizer, test_dataset)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
