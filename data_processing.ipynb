{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import functions as fct\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins vers les fichiers JSON d'entraînement et de développement\n",
    "train_path = 'SCOTUS/train.json'\n",
    "dev_path = 'SCOTUS/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données d'entraînement à partir du fichier JSON\n",
    "with open(train_path, 'r', encoding=\"utf-8\") as f:\n",
    "    train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_processing(html_content):\n",
    "    \"\"\"\n",
    "    Récupère le contenu intéressant du HTML en conservant les balises <a>, <em> et <blockquote>.\n",
    "    \n",
    "    Parameters:\n",
    "    html_content (str): Le contenu HTML à traiter.\n",
    "\n",
    "    Returns:\n",
    "    list: Une liste de chaînes contenant le texte traité et les balises spécifiées.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # On suppose que le contenu intéressant se trouve dans cette classe\n",
    "    div_class = \"-display-inline-block text-left\"\n",
    "    target_div = soup.find('div', class_=div_class)\n",
    "    \n",
    "    extract = []\n",
    "\n",
    "    temp_result = []  # Liste pour stocker le texte traité\n",
    "    if target_div:\n",
    "        to_merge = False  # Indicateur pour savoir si on doit fusionner les textes\n",
    "        for element in target_div.children:\n",
    "            # Ignorer les sauts de ligne\n",
    "            if element.getText() == '\\n':\n",
    "                continue\n",
    "            \n",
    "            extract.append(element.get_text())\n",
    "            \n",
    "            # Traiter les balises <a> et <em>\n",
    "            if element.name in [\"a\", \"em\"]:\n",
    "                # temp_result.append(f\"<{element.name}> {element.get_text()} </{element.name}>\")\n",
    "                temp_result.append(f\"{element.get_text()}\")\n",
    "                to_merge = True\n",
    "            # Si l'élément est du texte à fusionner\n",
    "            elif to_merge:\n",
    "                if temp_result[-1].startswith(\"<em>\"):\n",
    "                    temp_result[-2] += temp_result[-1] + element.get_text()\n",
    "                else:\n",
    "                    temp_result[-2] += \" \" + temp_result[-1] + \" \" + element.get_text()\n",
    "                temp_result.pop()  # Retirer le dernier élément\n",
    "                to_merge = False\n",
    "            # Traiter les balises <blockquote>\n",
    "            elif element.name == \"blockquote\":\n",
    "                # temp_result.append(f\"<{element.name}>{element.get_text()}</{element.name}>\")\n",
    "                temp_result.append(f\"{element.get_text()}\")\n",
    "            # Pour le texte normal\n",
    "            else:\n",
    "                temp_result.append(element.get_text())\n",
    "    else:\n",
    "        print(f\"Aucun div trouvé avec la classe '{div_class}'.\")\n",
    "\n",
    "    result = []  # Liste finale pour le résultat\n",
    "\n",
    "    # Nettoyer et fusionner le texte dans le résultat final\n",
    "    for res in temp_result:\n",
    "        # Remplacer les tabulations et les sauts de ligne\n",
    "        if '\\t' in res:\n",
    "            string = res.replace('\\t', '').replace('\\n', ' ')\n",
    "            result.append(string)\n",
    "        else:\n",
    "            if len(result) < 1:\n",
    "                result.append(res.replace('\\n', ''))\n",
    "            else:\n",
    "                result[-1] += \" \" + res.replace('\\n', '')\n",
    "\n",
    "    return result, extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commented so we don't overwrite the txt files by accident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement des données d'entraînement\n",
    "for i in range(len(train)):\n",
    "    html_content = train[i][\"raw_source\"]  # Récupérer le contenu HTML\n",
    "\n",
    "    text, _ = html_processing(html_content)  # Traiter le contenu HTML\n",
    "\n",
    "    name = f'data_txt_save/text/train_{i}.txt'  # Nom du fichier de sortie\n",
    "\n",
    "    # Écrire le texte traité dans un fichier\n",
    "    with open(name, 'w', encoding='utf-8') as f:\n",
    "        for line in text:\n",
    "            f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    tmp = train[i][\"raw_target\"]\n",
    "    text = tmp[\"facts_of_the_case\"] + \" \\n\" + tmp[\"question\"] + \" \\n\" + tmp[\"conclusion\"]\n",
    "\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    clean_text = soup.get_text()\n",
    "\n",
    "    name = f'data_txt_save/summary/train_{i}.txt'  # Nom du fichier de sortie\n",
    "\n",
    "    # Écrire le texte traité dans un fichier\n",
    "    with open(name, 'w', encoding='utf-8') as f:\n",
    "        for line in text:\n",
    "            f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_extract(extract_texts):\n",
    "    to_display = [] \n",
    "    for extract_text in extract_texts:\n",
    "        parts = extract_text.split('\\n')\n",
    "        to_display.extend(part.strip() for part in parts if part.strip()) \n",
    "        \n",
    "    return to_display  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the cleaned text in the html to see if every thing is extracted (but find only 1 occurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = train[0][\"raw_source\"]  # Récupérer le contenu HTML\n",
    "text, extract_texts = html_processing(html_content)\n",
    "\n",
    "to_display = split_extract(extract_texts)\n",
    "\n",
    "# we skip the first lines because the highlighting doesn't work well with the first html tags \n",
    "\n",
    "skipped_lines = 240 # where the syllabus start\n",
    "#skipped_lines = 375 # where the opinions start\n",
    "\n",
    "lines = html_content.splitlines()\n",
    "\n",
    "beginning = \"\\n\".join(lines[:skipped_lines])\n",
    "rest = \"\\n\".join(lines[skipped_lines:])\n",
    "\n",
    "highlighted_result = fct.highlight_html(rest, to_display)\n",
    "\n",
    "display(HTML(beginning+highlighted_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partially cleaned with cleaned is not working whelle because not handeling the \\<a\\> and \\<em\\> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_cleaned = fct.open_file(\"data_txt_save/text/train_0.txt\", 'txt')\n",
    "cleaned = fct.open_file(\"clean_data_txt_save/text/train_0.txt\", 'txt')\n",
    "\n",
    "extracts = [phrase.strip() for phrase in cleaned.split('\\n') if phrase.strip()]\n",
    "\n",
    "\n",
    "highlighted_result_2 = fct.highlight_text(not_cleaned, extracts)\n",
    "\n",
    "display(HTML(f\"<p>{highlighted_result_2}</p>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
