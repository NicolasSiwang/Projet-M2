{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (4.27.2)\n",
      "Requirement already satisfied: torch in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (2.4.1+cu124)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nicolas\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~illow (c:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~illow (c:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~illow (c:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas tqdm transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS_COUNT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LexRank_data_path = '../evaluate models/output/results_Luhn_dev.csv'\n",
    "target_path_csv = '../SCOTUS_data/paragraph_target_df_dev.csv'\n",
    "\n",
    "df = pd.read_csv(LexRank_data_path)\n",
    "df_target = pd.read_csv(target_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OCTOBER TERM, 2002\\n  Syllabus\\n  EARLY, WA...</td>\n",
       "      <td>Acting on a tip from a confidential informant ...</td>\n",
       "      <td>On direct appeal, the State Court of Appeal re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OCTOBER TERM, 2002\\n  Syllabus\\n  DOW CHEMI...</td>\n",
       "      <td>Robert Smith was convicted of first-degree mur...</td>\n",
       "      <td>Syllabus DOW CHEMICAL CO.\\nET AL.  v.  STEPHEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OCTOBER TERM, 2002\\n  Syllabus\\n  SYNGENTA ...</td>\n",
       "      <td>In 1984, Congress enacted legislation ordering...</td>\n",
       "      <td>Argued October 15, 2002-Decided November 5, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OPINION OF THE COURTRUMSFELD V. PADILLA542 U. ...</td>\n",
       "      <td>Stoneridge Investment Partners alleged that th...</td>\n",
       "      <td>DONALD H. RUMSFELD, SECRETARY OF DEFENSE, PETI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OCTOBER TERM, 1993\\n  Syllabus\\n  CONSOLIDA...</td>\n",
       "      <td>Consolidated Rail Corporation (Conrail) employ...</td>\n",
       "      <td>Finally, the common law does not support the c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0     OCTOBER TERM, 2002\\n  Syllabus\\n  EARLY, WA...   \n",
       "1     OCTOBER TERM, 2002\\n  Syllabus\\n  DOW CHEMI...   \n",
       "2     OCTOBER TERM, 2002\\n  Syllabus\\n  SYNGENTA ...   \n",
       "3  OPINION OF THE COURTRUMSFELD V. PADILLA542 U. ...   \n",
       "4     OCTOBER TERM, 1993\\n  Syllabus\\n  CONSOLIDA...   \n",
       "\n",
       "                                           Reference  \\\n",
       "0  Acting on a tip from a confidential informant ...   \n",
       "1  Robert Smith was convicted of first-degree mur...   \n",
       "2  In 1984, Congress enacted legislation ordering...   \n",
       "3  Stoneridge Investment Partners alleged that th...   \n",
       "4  Consolidated Rail Corporation (Conrail) employ...   \n",
       "\n",
       "                                           Generated  \n",
       "0  On direct appeal, the State Court of Appeal re...  \n",
       "1  Syllabus DOW CHEMICAL CO.\\nET AL.  v.  STEPHEN...  \n",
       "2  Argued October 15, 2002-Decided November 5, 20...  \n",
       "3  DONALD H. RUMSFELD, SECRETARY OF DEFENSE, PETI...  \n",
       "4  Finally, the common law does not support the c...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_txt = df['Generated']\n",
    "ref_txt = df['Reference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'logging' from 'huggingface_hub' (c:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\huggingface_hub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Détection de l'appareil\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     logging,\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     46\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\transformers\\dependency_versions_check.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     26\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython tqdm regex requests packaging filelock numpy tokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[1;32mc:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\transformers\\utils\\__init__.py:30\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     24\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     29\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     ContextManagers,\n\u001b[0;32m     32\u001b[0m     ExplicitEnum,\n\u001b[0;32m     33\u001b[0m     ModelOutput,\n\u001b[0;32m     34\u001b[0m     PaddingStrategy,\n\u001b[0;32m     35\u001b[0m     TensorType,\n\u001b[0;32m     36\u001b[0m     cached_property,\n\u001b[0;32m     37\u001b[0m     can_return_loss,\n\u001b[0;32m     38\u001b[0m     expand_dims,\n\u001b[0;32m     39\u001b[0m     find_labels,\n\u001b[0;32m     40\u001b[0m     flatten_dict,\n\u001b[0;32m     41\u001b[0m     is_jax_tensor,\n\u001b[0;32m     42\u001b[0m     is_numpy_array,\n\u001b[0;32m     43\u001b[0m     is_tensor,\n\u001b[0;32m     44\u001b[0m     is_tf_tensor,\n\u001b[0;32m     45\u001b[0m     is_torch_device,\n\u001b[0;32m     46\u001b[0m     is_torch_dtype,\n\u001b[0;32m     47\u001b[0m     is_torch_tensor,\n\u001b[0;32m     48\u001b[0m     reshape,\n\u001b[0;32m     49\u001b[0m     squeeze,\n\u001b[0;32m     50\u001b[0m     tensor_size,\n\u001b[0;32m     51\u001b[0m     to_numpy,\n\u001b[0;32m     52\u001b[0m     to_py_obj,\n\u001b[0;32m     53\u001b[0m     transpose,\n\u001b[0;32m     54\u001b[0m     working_or_temp_dir,\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     57\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[0;32m     58\u001b[0m     DISABLE_TELEMETRY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     send_example_telemetry,\n\u001b[0;32m     85\u001b[0m )\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     87\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[0;32m     88\u001b[0m     ENV_VARS_TRUE_VALUES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m     torch_version,\n\u001b[0;32m    167\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:29\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, ContextManager, List, Tuple\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_flax_available, is_tf_available, is_torch_available, is_torch_fx_proxy\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:32\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m importlib_metadata\n\u001b[0;32m     36\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\transformers\\utils\\logging.py:35\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     CRITICAL,  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     DEBUG,  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     WARNING,  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhf_hub_utils\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto \u001b[38;5;28;01mas\u001b[39;00m tqdm_lib\n\u001b[0;32m     39\u001b[0m _lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n",
      "File \u001b[1;32mc:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\__init__.py:93\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     81\u001b[0m     HFValidationError,\n\u001b[0;32m     82\u001b[0m     smoothly_deprecate_use_auth_token,\n\u001b[0;32m     83\u001b[0m     validate_hf_hub_args,\n\u001b[0;32m     84\u001b[0m     validate_repo_id,\n\u001b[0;32m     85\u001b[0m )\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     87\u001b[0m     are_progress_bars_disabled,\n\u001b[0;32m     88\u001b[0m     disable_progress_bars,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m     tqdm_stream_file,\n\u001b[0;32m     92\u001b[0m )\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_telemetry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m send_telemetry\n",
      "File \u001b[1;32mc:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_telemetry.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quote\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants, logging\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_hf_headers, get_session, hf_raise_for_status\n\u001b[0;32m     10\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'logging' from 'huggingface_hub' (c:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\huggingface_hub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Détection de l'appareil\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Charger le tokenizer et le modèle\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nsi319/legal-pegasus\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"nsi319/legal-pegasus\").to(device)\n",
    "\n",
    "# Fonction pour résumer les textes\n",
    "def summarize_texts(texts):\n",
    "    summaries = []\n",
    "    for text in texts:\n",
    "        # Préparer l'entrée pour le modèle\n",
    "        inputs = tokenizer(text, max_length=1024, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Générer le résumé\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=1024,\n",
    "            min_length=40,\n",
    "            length_penalty=2.0,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Décoder le résumé généré\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Résumer les textes dans gen_txt\n",
    "summarized_texts = summarize_texts(gen_txt)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Ajouter les résumés au DataFrame\n",
    "df['Summary'] = summarized_texts\n",
    "\n",
    "df.to_csv('./output/results_Hybrid_Luhn_Legal_dev.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_gen = df['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores ROUGE : {'rouge1': 0.36536931894440317, 'rouge2': 0.1199662001046805, 'rougeL': 0.22142622385237762, 'bert_score': 0.8255935955047607}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "ROUGE_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "BERT_scorer = BERTScorer(lang=\"en\")\n",
    "ROUGE_scores = []\n",
    "BERT_scores = []\n",
    "for i in range(len(ref_txt)):\n",
    "    score = ROUGE_scorer.score(ref_txt[i], summary_gen[i])\n",
    "    ROUGE_scores.append(score)\n",
    "    BERT_scores.append(BERT_scorer.score([ref_txt[i]], [summary_gen[i]]))\n",
    "    # print(f\"Scores pour le résumé {i+1} :\", score)\n",
    "\n",
    "# Moyennes des scores\n",
    "avg_scores = {\n",
    "    'rouge1': sum(s['rouge1'].recall for s in ROUGE_scores) / len(ROUGE_scores),\n",
    "    'rouge2': sum(s['rouge2'].recall for s in ROUGE_scores) / len(ROUGE_scores),\n",
    "    'rougeL': sum(s['rougeL'].recall for s in ROUGE_scores) / len(ROUGE_scores),\n",
    "    'bert_score': sum(s[2].mean().item() for s in BERT_scores) / len(BERT_scores),\n",
    "}\n",
    "\n",
    "print(\"Scores ROUGE :\", avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores moyens pour facts_of_the_case :\n",
      "{'rouge1': 0.306481537977766, 'rouge2': 0.04488918383936519, 'rougeL': 0.21236966189969575, 'bert_score': 0.7978615707159042}\n",
      "\n",
      "Scores moyens pour question :\n",
      "{'rouge1': 0.34774350403739335, 'rouge2': 0.05005823144621627, 'rougeL': 0.29334381862027237, 'bert_score': 0.7922924607992172}\n",
      "\n",
      "Scores moyens pour conclusion :\n",
      "{'rouge1': 0.31080339758491965, 'rouge2': 0.05310137414274551, 'rougeL': 0.21834438949550697, 'bert_score': 0.7979495960474015}\n"
     ]
    }
   ],
   "source": [
    "scores = {\n",
    "    'facts_of_the_case': {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bert_score': []},\n",
    "    'question': {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bert_score': []},\n",
    "    'conclusion': {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bert_score': []}\n",
    "}\n",
    "\n",
    "for column_name in df_target.columns:\n",
    "    for i in range(TEXTS_COUNT):\n",
    "        ref = df_target[column_name].iloc[i]\n",
    "        gen = summary_gen[i]\n",
    "\n",
    "        # Scores ROUGE\n",
    "        rouge_score = ROUGE_scorer.score(ref, gen)\n",
    "        scores[column_name]['rouge1'].append(rouge_score['rouge1'].recall)\n",
    "        scores[column_name]['rouge2'].append(rouge_score['rouge2'].recall)\n",
    "        scores[column_name]['rougeL'].append(rouge_score['rougeL'].recall)\n",
    "\n",
    "        # Scores BERT\n",
    "        _, _, bert_score = BERT_scorer.score([ref], [gen])\n",
    "        scores[column_name]['bert_score'].append(bert_score.mean().item())\n",
    "\n",
    "avg_scores_target = {\n",
    "    col: {\n",
    "        'rouge1': sum(scores[col]['rouge1']) / len(scores[col]['rouge1']),\n",
    "        'rouge2': sum(scores[col]['rouge2']) / len(scores[col]['rouge2']),\n",
    "        'rougeL': sum(scores[col]['rougeL']) / len(scores[col]['rougeL']),\n",
    "        'bert_score': sum(scores[col]['bert_score']) / len(scores[col]['bert_score'])\n",
    "    }\n",
    "    for col in df_target.columns\n",
    "}\n",
    "\n",
    "for col, metrics in avg_scores_target.items():\n",
    "    print(f\"\\nScores moyens pour {col} :\")\n",
    "    print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     rouge1    rouge2    rougeL  bert_score  Execution time\n",
      "global             0.365369  0.119966  0.221426    0.825594      2004.62402\n",
      "facts_of_the_case  0.306482  0.044889  0.212370    0.797862      2004.62402\n",
      "question           0.347744  0.050058  0.293344    0.792292      2004.62402\n",
      "conclusion         0.310803  0.053101  0.218344    0.797950      2004.62402\n"
     ]
    }
   ],
   "source": [
    "df_avg_scores = pd.DataFrame([avg_scores])\n",
    "df_avg_scores.index = ['global']\n",
    "\n",
    "df_avg_scores_target = pd.DataFrame(avg_scores_target).T\n",
    "\n",
    "df_score = pd.concat([df_avg_scores, df_avg_scores_target], axis=0)\n",
    "df_score['Execution time'] = execution_time\n",
    "\n",
    "print(df_score.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.to_csv(\"./output/scores_Hybrid_Luhn_Legal_dev.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
