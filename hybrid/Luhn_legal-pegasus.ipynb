{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS_COUNT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Luhn_data_path = '../evaluate models/output/results_Luhn_dev.csv'\n",
    "target_path_csv = '../SCOTUS_data/paragraph_target_df_dev.csv'\n",
    "\n",
    "df = pd.read_csv(Luhn_data_path)\n",
    "df_target = pd.read_csv(target_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OCTOBER TERM, 2002\\n  Syllabus\\n  EARLY, WA...</td>\n",
       "      <td>William Packer was convicted in a California s...</td>\n",
       "      <td>On direct appeal, the State Court of Appeal re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OCTOBER TERM, 2002\\n  Syllabus\\n  DOW CHEMI...</td>\n",
       "      <td>In 1984 Dow Chemical Co. negotiated a settleme...</td>\n",
       "      <td>Syllabus DOW CHEMICAL CO.\\nET AL.  v.  STEPHEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OCTOBER TERM, 2002\\n  Syllabus\\n  SYNGENTA ...</td>\n",
       "      <td>Hurley Henson filed suit in Louisiana state co...</td>\n",
       "      <td>Argued October 15, 2002-Decided November 5, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OPINION OF THE COURTRUMSFELD V. PADILLA542 U. ...</td>\n",
       "      <td>Jose Padilla, an American citizen, was arreste...</td>\n",
       "      <td>DONALD H. RUMSFELD, SECRETARY OF DEFENSE, PETI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OCTOBER TERM, 1993\\n  Syllabus\\n  CONSOLIDA...</td>\n",
       "      <td>Consolidated Rail Corporation (Conrail) employ...</td>\n",
       "      <td>Finally, the common law does not support the c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0     OCTOBER TERM, 2002\\n  Syllabus\\n  EARLY, WA...   \n",
       "1     OCTOBER TERM, 2002\\n  Syllabus\\n  DOW CHEMI...   \n",
       "2     OCTOBER TERM, 2002\\n  Syllabus\\n  SYNGENTA ...   \n",
       "3  OPINION OF THE COURTRUMSFELD V. PADILLA542 U. ...   \n",
       "4     OCTOBER TERM, 1993\\n  Syllabus\\n  CONSOLIDA...   \n",
       "\n",
       "                                           Reference  \\\n",
       "0  William Packer was convicted in a California s...   \n",
       "1  In 1984 Dow Chemical Co. negotiated a settleme...   \n",
       "2  Hurley Henson filed suit in Louisiana state co...   \n",
       "3  Jose Padilla, an American citizen, was arreste...   \n",
       "4  Consolidated Rail Corporation (Conrail) employ...   \n",
       "\n",
       "                                           Generated  \n",
       "0  On direct appeal, the State Court of Appeal re...  \n",
       "1  Syllabus DOW CHEMICAL CO.\\nET AL.  v.  STEPHEN...  \n",
       "2  Argued October 15, 2002-Decided November 5, 20...  \n",
       "3  DONALD H. RUMSFELD, SECRETARY OF DEFENSE, PETI...  \n",
       "4  Finally, the common law does not support the c...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_txt = df['Generated']\n",
    "ref_txt = df['Reference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elisa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elisa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Détection de l'appareil\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Charger le tokenizer et le modèle\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nsi319/legal-pegasus\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"nsi319/legal-pegasus\").to(device)\n",
    "\n",
    "# Fonction pour résumer les textes\n",
    "def summarize_texts(texts):\n",
    "    summaries = []\n",
    "    for text in texts:\n",
    "        # Préparer l'entrée pour le modèle\n",
    "        inputs = tokenizer(text, max_length=1024, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Générer le résumé\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=1024,\n",
    "            min_length=40,\n",
    "            length_penalty=2.0,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Décoder le résumé généré\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Résumer les textes dans gen_txt\n",
    "summarized_texts = summarize_texts(gen_txt)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Ajouter les résumés au DataFrame\n",
    "df['Summary'] = summarized_texts\n",
    "\n",
    "df.to_csv('./output/results_Hybrid_Luhn_Legal_dev.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_gen = df['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elisa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores ROUGE : {'rouge1': 0.3818866574207332, 'rouge2': 0.131564117911718, 'rougeL': 0.2303390604824208, 'bert_score': 0.828329815864563}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "ROUGE_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "BERT_scorer = BERTScorer(lang=\"en\")\n",
    "ROUGE_scores = []\n",
    "BERT_scores = []\n",
    "for i in range(len(ref_txt)):\n",
    "    score = ROUGE_scorer.score(ref_txt[i], summary_gen[i])\n",
    "    ROUGE_scores.append(score)\n",
    "    BERT_scores.append(BERT_scorer.score([ref_txt[i]], [summary_gen[i]]))\n",
    "    # print(f\"Scores pour le résumé {i+1} :\", score)\n",
    "\n",
    "# Moyennes des scores\n",
    "avg_scores = {\n",
    "    'rouge1': sum(s['rouge1'].recall for s in ROUGE_scores) / len(ROUGE_scores),\n",
    "    'rouge2': sum(s['rouge2'].recall for s in ROUGE_scores) / len(ROUGE_scores),\n",
    "    'rougeL': sum(s['rougeL'].recall for s in ROUGE_scores) / len(ROUGE_scores),\n",
    "    'bert_score': sum(s[2].mean().item() for s in BERT_scores) / len(BERT_scores),\n",
    "}\n",
    "\n",
    "print(\"Scores ROUGE :\", avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores moyens pour facts_of_the_case :\n",
      "{'rouge1': 0.30606465131308896, 'rouge2': 0.04463438016350889, 'rougeL': 0.2125767278531862, 'bert_score': 0.797832015156746}\n",
      "\n",
      "Scores moyens pour question :\n",
      "{'rouge1': 0.3476641389580283, 'rouge2': 0.051058231446216275, 'rougeL': 0.29435572338217714, 'bert_score': 0.7922170692682267}\n",
      "\n",
      "Scores moyens pour conclusion :\n",
      "{'rouge1': 0.3108607146580904, 'rouge2': 0.0534851168674175, 'rougeL': 0.21831780412965332, 'bert_score': 0.79785320520401}\n"
     ]
    }
   ],
   "source": [
    "scores = {\n",
    "    'facts_of_the_case': {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bert_score': []},\n",
    "    'question': {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bert_score': []},\n",
    "    'conclusion': {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bert_score': []}\n",
    "}\n",
    "\n",
    "for column_name in df_target.columns:\n",
    "    for i in range(TEXTS_COUNT):\n",
    "        ref = df_target[column_name].iloc[i]\n",
    "        gen = summary_gen[i]\n",
    "\n",
    "        # Scores ROUGE\n",
    "        rouge_score = ROUGE_scorer.score(ref, gen)\n",
    "        scores[column_name]['rouge1'].append(rouge_score['rouge1'].recall)\n",
    "        scores[column_name]['rouge2'].append(rouge_score['rouge2'].recall)\n",
    "        scores[column_name]['rougeL'].append(rouge_score['rougeL'].recall)\n",
    "\n",
    "        # Scores BERT\n",
    "        _, _, bert_score = BERT_scorer.score([ref], [gen])\n",
    "        scores[column_name]['bert_score'].append(bert_score.mean().item())\n",
    "\n",
    "avg_scores_target = {\n",
    "    col: {\n",
    "        'rouge1': sum(scores[col]['rouge1']) / len(scores[col]['rouge1']),\n",
    "        'rouge2': sum(scores[col]['rouge2']) / len(scores[col]['rouge2']),\n",
    "        'rougeL': sum(scores[col]['rougeL']) / len(scores[col]['rougeL']),\n",
    "        'bert_score': sum(scores[col]['bert_score']) / len(scores[col]['bert_score'])\n",
    "    }\n",
    "    for col in df_target.columns\n",
    "}\n",
    "\n",
    "for col, metrics in avg_scores_target.items():\n",
    "    print(f\"\\nScores moyens pour {col} :\")\n",
    "    print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     rouge1    rouge2    rougeL  bert_score  Execution time\n",
      "global             0.381887  0.131564  0.230339    0.828330      834.282983\n",
      "facts_of_the_case  0.306065  0.044634  0.212577    0.797832      834.282983\n",
      "question           0.347664  0.051058  0.294356    0.792217      834.282983\n",
      "conclusion         0.310861  0.053485  0.218318    0.797853      834.282983\n"
     ]
    }
   ],
   "source": [
    "df_avg_scores = pd.DataFrame([avg_scores])\n",
    "df_avg_scores.index = ['global']\n",
    "\n",
    "df_avg_scores_target = pd.DataFrame(avg_scores_target).T\n",
    "\n",
    "df_score = pd.concat([df_avg_scores, df_avg_scores_target], axis=0)\n",
    "df_score['Execution time'] = execution_time\n",
    "\n",
    "print(df_score.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.to_csv(\"./output/scores_Hybrid_Luhn_Legal_dev.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
