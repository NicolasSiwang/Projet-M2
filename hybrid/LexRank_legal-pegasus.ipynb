{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS_COUNT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LexRank_data_path = '../resultats/results_LexRank.csv'\n",
    "target_path_csv = '../SCOTUS_data/paragraph_target_df_dev.csv'\n",
    "\n",
    "df = pd.read_csv(LexRank_data_path)\n",
    "df_target = pd.read_csv(target_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OCTOBER TERM, 2002\\n  Syllabus\\n  EARLY, WA...</td>\n",
       "      <td>William Packer was convicted in a California s...</td>\n",
       "      <td>On direct appeal, the State Court of Appeal re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OCTOBER TERM, 2002\\n  Syllabus\\n  DOW CHEMI...</td>\n",
       "      <td>In 1984 Dow Chemical Co. negotiated a settleme...</td>\n",
       "      <td>Syllabus DOW CHEMICAL CO.\\nArgued February 26,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OCTOBER TERM, 2002\\n  Syllabus\\n  SYNGENTA ...</td>\n",
       "      <td>Hurley Henson filed suit in Louisiana state co...</td>\n",
       "      <td>Argued October 15, 2002-Decided November 5, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OPINION OF THE COURTRUMSFELD V. PADILLA542 U. ...</td>\n",
       "      <td>Jose Padilla, an American citizen, was arreste...</td>\n",
       "      <td>Padilla’s motion was still pending when, on Ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OCTOBER TERM, 1993\\n  Syllabus\\n  CONSOLIDA...</td>\n",
       "      <td>Consolidated Rail Corporation (Conrail) employ...</td>\n",
       "      <td>The injury we contemplate when considering neg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0     OCTOBER TERM, 2002\\n  Syllabus\\n  EARLY, WA...   \n",
       "1     OCTOBER TERM, 2002\\n  Syllabus\\n  DOW CHEMI...   \n",
       "2     OCTOBER TERM, 2002\\n  Syllabus\\n  SYNGENTA ...   \n",
       "3  OPINION OF THE COURTRUMSFELD V. PADILLA542 U. ...   \n",
       "4     OCTOBER TERM, 1993\\n  Syllabus\\n  CONSOLIDA...   \n",
       "\n",
       "                                           Reference  \\\n",
       "0  William Packer was convicted in a California s...   \n",
       "1  In 1984 Dow Chemical Co. negotiated a settleme...   \n",
       "2  Hurley Henson filed suit in Louisiana state co...   \n",
       "3  Jose Padilla, an American citizen, was arreste...   \n",
       "4  Consolidated Rail Corporation (Conrail) employ...   \n",
       "\n",
       "                                           Generated  \n",
       "0  On direct appeal, the State Court of Appeal re...  \n",
       "1  Syllabus DOW CHEMICAL CO.\\nArgued February 26,...  \n",
       "2  Argued October 15, 2002-Decided November 5, 20...  \n",
       "3  Padilla’s motion was still pending when, on Ju...  \n",
       "4  The injury we contemplate when considering neg...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "gen_txt = df['Generated']\n",
    "ref_txt = df['Reference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Détection de l'appareil\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Charger le tokenizer et le modèle\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nsi319/legal-pegasus\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"nsi319/legal-pegasus\").to(device)\n",
    "\n",
    "# Fonction pour résumer les textes\n",
    "def summarize_texts(texts):\n",
    "    summaries = []\n",
    "    for text in texts:\n",
    "        # Préparer l'entrée pour le modèle\n",
    "        inputs = tokenizer(text, max_length=1024, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Générer le résumé\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=1024,\n",
    "            min_length=40,\n",
    "            length_penalty=2.0,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Décoder le résumé généré\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries\n",
    "\n",
    "# Résumer les textes dans gen_txt\n",
    "summarized_texts = summarize_texts(gen_txt)\n",
    "\n",
    "# Ajouter les résumés au DataFrame\n",
    "df['Summary'] = summarized_texts\n",
    "\n",
    "df.to_csv('./output/results_Hybrid_Lex_Legal.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_gen = df['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "ROUGE_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "BERT_scorer = BERTScorer(lang=\"en\")\n",
    "ROUGE_scores = []\n",
    "BERT_scores = []\n",
    "for i in range(len(ref_txt)):\n",
    "    score = ROUGE_scorer.score(ref_txt[i], summary_gen[i])\n",
    "    ROUGE_scores.append(score)\n",
    "    BERT_scores.append(BERT_scorer.score([ref_txt[i]], [summary_gen[i]]))\n",
    "    # print(f\"Scores pour le résumé {i+1} :\", score)\n",
    "\n",
    "# Moyennes des scores\n",
    "avg_scores = {\n",
    "    'rouge1': sum(s['rouge1'].fmeasure for s in ROUGE_scores) / len(ROUGE_scores),\n",
    "    'rouge2': sum(s['rouge2'].fmeasure for s in ROUGE_scores) / len(ROUGE_scores),\n",
    "    'rougeL': sum(s['rougeL'].fmeasure for s in ROUGE_scores) / len(ROUGE_scores),\n",
    "    'bert_score': sum(s[2].mean().item() for s in BERT_scores) / len(BERT_scores),\n",
    "}\n",
    "\n",
    "print(\"Scores ROUGE :\", avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    'facts_of_the_case': {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bert_score': []},\n",
    "    'question': {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bert_score': []},\n",
    "    'conclusion': {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bert_score': []}\n",
    "}\n",
    "\n",
    "for column_name in df_target.columns:\n",
    "    for i in range(TEXTS_COUNT):\n",
    "        ref = df_target[column_name].iloc[i]\n",
    "        gen = summary_gen[i]\n",
    "\n",
    "        # Scores ROUGE\n",
    "        rouge_score = ROUGE_scorer.score(ref, gen)\n",
    "        scores[column_name]['rouge1'].append(rouge_score['rouge1'].recall)\n",
    "        scores[column_name]['rouge2'].append(rouge_score['rouge2'].recall)\n",
    "        scores[column_name]['rougeL'].append(rouge_score['rougeL'].recall)\n",
    "\n",
    "        # Scores BERT\n",
    "        _, _, bert_score = BERT_scorer.score([ref], [gen])\n",
    "        scores[column_name]['bert_score'].append(bert_score.mean().item())\n",
    "\n",
    "avg_scores_target = {\n",
    "    col: {\n",
    "        'rouge1': sum(scores[col]['rouge1']) / len(scores[col]['rouge1']),\n",
    "        'rouge2': sum(scores[col]['rouge2']) / len(scores[col]['rouge2']),\n",
    "        'rougeL': sum(scores[col]['rougeL']) / len(scores[col]['rougeL']),\n",
    "        'bert_score': sum(scores[col]['bert_score']) / len(scores[col]['bert_score'])\n",
    "    }\n",
    "    for col in df_target.columns\n",
    "}\n",
    "\n",
    "for col, metrics in avg_scores_target.items():\n",
    "    print(f\"\\nScores moyens pour {col} :\")\n",
    "    print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_scores = pd.DataFrame([avg_scores])\n",
    "df_avg_scores.index = ['global']\n",
    "\n",
    "df_avg_scores_target = pd.DataFrame(avg_scores_target).T\n",
    "\n",
    "df_score = pd.concat([df_avg_scores, df_avg_scores_target], axis=0)\n",
    "print(df_score.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.to_csv(\"./output/scores_Hybrid_Lex_Legal_dev.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
