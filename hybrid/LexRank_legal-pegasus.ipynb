{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS_COUNT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LexRank_data_path = '../evaluate models/output/results_LexRank_dev.csv'\n",
    "target_path_csv = '../SCOTUS_data/paragraph_target_df_dev.csv'\n",
    "\n",
    "df = pd.read_csv(LexRank_data_path)\n",
    "df_target = pd.read_csv(target_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OCTOBER TERM, 1998\\n  Per Curiam \\n  MARYLA...</td>\n",
       "      <td>Acting on a tip from a confidential informant ...</td>\n",
       "      <td>The Court of Special Appeals reversed his drug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OCTOBER TERM, 2001\\n  Syllabus\\n  STEWART, ...</td>\n",
       "      <td>Robert Smith was convicted of first-degree mur...</td>\n",
       "      <td>He had previously brought that claim in a stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. Supreme Court South Dakota v. Dole, 483 U...</td>\n",
       "      <td>In 1984, Congress enacted legislation ordering...</td>\n",
       "      <td>Held:  Even if Congress, in view of the Twenty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OPINION OF THE COURTSTONERIDGE INVESTMENT PART...</td>\n",
       "      <td>Stoneridge Investment Partners alleged that th...</td>\n",
       "      <td>STONERIDGE INVESTMENT PARTNERS, LLC, PETITIONE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OCTOBER TERM, 1999\\n  Syllabus\\n  FISCHER  ...</td>\n",
       "      <td>Jeffrey Fischer, while president and part owne...</td>\n",
       "      <td>The Court rejects petitioner's argument that M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0     OCTOBER TERM, 1998\\n  Per Curiam \\n  MARYLA...   \n",
       "1     OCTOBER TERM, 2001\\n  Syllabus\\n  STEWART, ...   \n",
       "2  U.S. Supreme Court South Dakota v. Dole, 483 U...   \n",
       "3  OPINION OF THE COURTSTONERIDGE INVESTMENT PART...   \n",
       "4     OCTOBER TERM, 1999\\n  Syllabus\\n  FISCHER  ...   \n",
       "\n",
       "                                           Reference  \\\n",
       "0  Acting on a tip from a confidential informant ...   \n",
       "1  Robert Smith was convicted of first-degree mur...   \n",
       "2  In 1984, Congress enacted legislation ordering...   \n",
       "3  Stoneridge Investment Partners alleged that th...   \n",
       "4  Jeffrey Fischer, while president and part owne...   \n",
       "\n",
       "                                           Generated  \n",
       "0  The Court of Special Appeals reversed his drug...  \n",
       "1  He had previously brought that claim in a stat...  \n",
       "2  Held:  Even if Congress, in view of the Twenty...  \n",
       "3  STONERIDGE INVESTMENT PARTNERS, LLC, PETITIONE...  \n",
       "4  The Court rejects petitioner's argument that M...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_txt = df['Generated']\n",
    "ref_txt = df['Reference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Détection de l'appareil\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Charger le tokenizer et le modèle\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nsi319/legal-pegasus\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"nsi319/legal-pegasus\").to(device)\n",
    "\n",
    "# Fonction pour résumer les textes\n",
    "def summarize_texts(texts):\n",
    "    summaries = []\n",
    "    for text in texts:\n",
    "        # Préparer l'entrée pour le modèle\n",
    "        inputs = tokenizer(text, max_length=1024, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Générer le résumé\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=1024,\n",
    "            min_length=40,\n",
    "            length_penalty=2.0,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Décoder le résumé généré\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Résumer les textes dans gen_txt\n",
    "summarized_texts = summarize_texts(gen_txt)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Ajouter les résumés au DataFrame\n",
    "df['Summary'] = summarized_texts\n",
    "\n",
    "df.to_csv('./output/results_Hybrid_Lex_Legal_dev.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_gen = df['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elisa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores ROUGE : {'rouge1': 0.32715603071858096, 'rouge2': 0.12186403507830633, 'rougeL': 0.18849373247809997, 'bert_score': 0.8367526334524155}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "ROUGE_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "BERT_scorer = BERTScorer(lang=\"en\")\n",
    "ROUGE_scores = []\n",
    "BERT_scores = []\n",
    "for i in range(len(ref_txt)):\n",
    "    score = ROUGE_scorer.score(ref_txt[i], summary_gen[i])\n",
    "    ROUGE_scores.append(score)\n",
    "    BERT_scores.append(BERT_scorer.score([ref_txt[i]], [summary_gen[i]]))\n",
    "    # print(f\"Scores pour le résumé {i+1} :\", score)\n",
    "\n",
    "# Moyennes des scores\n",
    "avg_scores = {\n",
    "    'rouge1': sum(s['rouge1'].recall for s in ROUGE_scores) / len(ROUGE_scores),\n",
    "    'rouge2': sum(s['rouge2'].recall for s in ROUGE_scores) / len(ROUGE_scores),\n",
    "    'rougeL': sum(s['rougeL'].recall for s in ROUGE_scores) / len(ROUGE_scores),\n",
    "    'bert_score': sum(s[2].mean().item() for s in BERT_scores) / len(BERT_scores),\n",
    "}\n",
    "\n",
    "print(\"Scores ROUGE :\", avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores moyens pour facts_of_the_case :\n",
      "{'rouge1': 0.276174342085735, 'rouge2': 0.046483085034165705, 'rougeL': 0.18270109571315257, 'bert_score': 0.8069009220600128}\n",
      "\n",
      "Scores moyens pour question :\n",
      "{'rouge1': 0.32862757702419637, 'rouge2': 0.04348068599596794, 'rougeL': 0.28103686785026666, 'bert_score': 0.8033384102582931}\n",
      "\n",
      "Scores moyens pour conclusion :\n",
      "{'rouge1': 0.28971128858000234, 'rouge2': 0.04942551038781437, 'rougeL': 0.1897257529854501, 'bert_score': 0.8077791267633438}\n"
     ]
    }
   ],
   "source": [
    "scores = {\n",
    "    'facts_of_the_case': {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bert_score': []},\n",
    "    'question': {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bert_score': []},\n",
    "    'conclusion': {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bert_score': []}\n",
    "}\n",
    "\n",
    "for column_name in df_target.columns:\n",
    "    for i in range(TEXTS_COUNT):\n",
    "        ref = df_target[column_name].iloc[i]\n",
    "        gen = summary_gen[i]\n",
    "\n",
    "        # Scores ROUGE\n",
    "        rouge_score = ROUGE_scorer.score(ref, gen)\n",
    "        scores[column_name]['rouge1'].append(rouge_score['rouge1'].recall)\n",
    "        scores[column_name]['rouge2'].append(rouge_score['rouge2'].recall)\n",
    "        scores[column_name]['rougeL'].append(rouge_score['rougeL'].recall)\n",
    "\n",
    "        # Scores BERT\n",
    "        _, _, bert_score = BERT_scorer.score([ref], [gen])\n",
    "        scores[column_name]['bert_score'].append(bert_score.mean().item())\n",
    "\n",
    "avg_scores_target = {\n",
    "    col: {\n",
    "        'rouge1': sum(scores[col]['rouge1']) / len(scores[col]['rouge1']),\n",
    "        'rouge2': sum(scores[col]['rouge2']) / len(scores[col]['rouge2']),\n",
    "        'rougeL': sum(scores[col]['rougeL']) / len(scores[col]['rougeL']),\n",
    "        'bert_score': sum(scores[col]['bert_score']) / len(scores[col]['bert_score'])\n",
    "    }\n",
    "    for col in df_target.columns\n",
    "}\n",
    "\n",
    "for col, metrics in avg_scores_target.items():\n",
    "    print(f\"\\nScores moyens pour {col} :\")\n",
    "    print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     rouge1    rouge2    rougeL  bert_score  Execution time\n",
      "global             0.327156  0.121864  0.188494    0.836753      574.897158\n",
      "facts_of_the_case  0.276174  0.046483  0.182701    0.806901      574.897158\n",
      "question           0.328628  0.043481  0.281037    0.803338      574.897158\n",
      "conclusion         0.289711  0.049426  0.189726    0.807779      574.897158\n"
     ]
    }
   ],
   "source": [
    "df_avg_scores = pd.DataFrame([avg_scores])\n",
    "df_avg_scores.index = ['global']\n",
    "\n",
    "df_avg_scores_target = pd.DataFrame(avg_scores_target).T\n",
    "\n",
    "df_score = pd.concat([df_avg_scores, df_avg_scores_target], axis=0)\n",
    "df_score['Execution time'] = execution_time\n",
    "\n",
    "print(df_score.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.to_csv(\"./output/scores_Hybrid_Lex_Legal_dev.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
