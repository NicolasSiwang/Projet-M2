{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LexRank_data_path = '../resultats/results_LexRank.csv'\n",
    "\n",
    "df = pd.read_csv(LexRank_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OCTOBER TERM, 2002\\n  Syllabus\\n  EARLY, WA...</td>\n",
       "      <td>William Packer was convicted in a California s...</td>\n",
       "      <td>On direct appeal, the State Court of Appeal re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OCTOBER TERM, 2002\\n  Syllabus\\n  DOW CHEMI...</td>\n",
       "      <td>In 1984 Dow Chemical Co. negotiated a settleme...</td>\n",
       "      <td>Syllabus DOW CHEMICAL CO.\\nArgued February 26,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OCTOBER TERM, 2002\\n  Syllabus\\n  SYNGENTA ...</td>\n",
       "      <td>Hurley Henson filed suit in Louisiana state co...</td>\n",
       "      <td>Argued October 15, 2002-Decided November 5, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OPINION OF THE COURTRUMSFELD V. PADILLA542 U. ...</td>\n",
       "      <td>Jose Padilla, an American citizen, was arreste...</td>\n",
       "      <td>Padilla’s motion was still pending when, on Ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OCTOBER TERM, 1993\\n  Syllabus\\n  CONSOLIDA...</td>\n",
       "      <td>Consolidated Rail Corporation (Conrail) employ...</td>\n",
       "      <td>The injury we contemplate when considering neg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0     OCTOBER TERM, 2002\\n  Syllabus\\n  EARLY, WA...   \n",
       "1     OCTOBER TERM, 2002\\n  Syllabus\\n  DOW CHEMI...   \n",
       "2     OCTOBER TERM, 2002\\n  Syllabus\\n  SYNGENTA ...   \n",
       "3  OPINION OF THE COURTRUMSFELD V. PADILLA542 U. ...   \n",
       "4     OCTOBER TERM, 1993\\n  Syllabus\\n  CONSOLIDA...   \n",
       "\n",
       "                                           Reference  \\\n",
       "0  William Packer was convicted in a California s...   \n",
       "1  In 1984 Dow Chemical Co. negotiated a settleme...   \n",
       "2  Hurley Henson filed suit in Louisiana state co...   \n",
       "3  Jose Padilla, an American citizen, was arreste...   \n",
       "4  Consolidated Rail Corporation (Conrail) employ...   \n",
       "\n",
       "                                           Generated  \n",
       "0  On direct appeal, the State Court of Appeal re...  \n",
       "1  Syllabus DOW CHEMICAL CO.\\nArgued February 26,...  \n",
       "2  Argued October 15, 2002-Decided November 5, 20...  \n",
       "3  Padilla’s motion was still pending when, on Ju...  \n",
       "4  The injury we contemplate when considering neg...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_txt = df['Generated']\n",
    "ref_txt = df['Reference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nsi319/legal-pegasus\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"nsi319/legal-pegasus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_summary(model, tokenizer, text, max_input_length=1024, max_output_length=256):\n",
    "    \"\"\"Génère un résumé pour un texte donné.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text, max_length=max_input_length, truncation=True, return_tensors=\"pt\", padding=\"longest\"\n",
    "    ).input_ids\n",
    "    inputs = inputs.to(model.device)  # S'assurer que les données sont sur le bon appareil\n",
    "\n",
    "    # Génération\n",
    "    output_ids = model.generate(\n",
    "        inputs, max_length=max_output_length, num_beams=5, length_penalty=2.0, early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores ROUGE et BERT : {'rouge1': 0.39637698047377723, 'rouge2': 0.13573539695952136, 'rougeL': 0.2187488646476557, 'bert_score': 0.02475793566321954}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "def evaluate_model(model, tokenizer, texts, references, max_input_length=1024, max_output_length=256):\n",
    "    \"\"\"Évalue les performances du modèle sur les données de test.\"\"\"\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    bert_scorer_instance = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    bert_scores_f1 = []\n",
    "    summaries = []\n",
    "\n",
    "    for text, reference in zip(texts, references):\n",
    "        # Générer le résumé\n",
    "        generated_summary = generate_summary(model, tokenizer, text, max_input_length, max_output_length)\n",
    "\n",
    "        summaries.append(generated_summary)\n",
    "\n",
    "        # Calcul des scores ROUGE\n",
    "        rouge_results = rouge_scorer_instance.score(reference, generated_summary)\n",
    "        rouge_scores['rouge1'].append(rouge_results['rouge1'].fmeasure)\n",
    "        rouge_scores['rouge2'].append(rouge_results['rouge2'].fmeasure)\n",
    "        rouge_scores['rougeL'].append(rouge_results['rougeL'].fmeasure)\n",
    "\n",
    "        # Calcul des scores BERT\n",
    "        _, _, f1 = bert_scorer_instance.score([generated_summary], [reference])\n",
    "        bert_scores_f1.append(f1.mean().item())  # Extraire la moyenne des scores F1\n",
    "\n",
    "    # Moyenne des scores\n",
    "    avg_scores = {\n",
    "        'rouge1': sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1']),\n",
    "        'rouge2': sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2']),\n",
    "        'rougeL': sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL']),\n",
    "        'bert_score': sum(bert_scores_f1) / len(bert_scores_f1),\n",
    "    }\n",
    "\n",
    "    return avg_scores, summaries\n",
    "\n",
    "# Évaluer le modèle\n",
    "results, summaries = evaluate_model(model, tokenizer, gen_txt, ref_txt)\n",
    "print(\"Scores ROUGE et BERT :\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary_df = pd.DataFrame({'Generated': gen_txt, 'Reference': ref_txt, 'Summary': summaries})\n",
    "summary_df.head()\n",
    "\n",
    "summary_df.to_csv('../resultats/results_LexRank_summaries.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
